{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_package(package: str):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "    \n",
    "\n",
    "def download_spacy_model(model: str):\n",
    "    subprocess.check_call([sys.executable, '-m', 'spacy', 'download', model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import collections\n",
    "import dataclasses\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from npdl.utils import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import npdl\n",
    "from npdl import activations\n",
    "from npdl import layers\n",
    "from npdl import objectives\n",
    "from npdl import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json', 'r') as file:\n",
    "    intents = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = str\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Document:\n",
    "    class_: str\n",
    "    tokens: Iterable[Token]\n",
    "\n",
    "def tokenize(sentence: str,\n",
    "             language: str = 'english') -> Iterable[Token]:\n",
    "    stop_words = ['?'] + stopwords.words(language)\n",
    "    return np.array([SnowballStemmer(language).stem(word.lower()) for word in nltk.word_tokenize(sentence, language)\n",
    "                     if word not in stop_words and word.isalpha()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the **vocabulary**, the **classes** and the **corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary: List[Token] = []\n",
    "corpus: List[Document] = []\n",
    "classes: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in intents:\n",
    "    class_ = document['class']\n",
    "    classes.append(class_)\n",
    "    for pattern in document['patterns']:\n",
    "        tokens = tokenize(pattern)\n",
    "        vocabulary.extend(tokens)\n",
    "        corpus.append(Document(class_, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = np.sort(np.unique(vocabulary))\n",
    "classes = np.sort(np.unique(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(train_labels, labels):\n",
    "    label_positions = {}\n",
    "    for i, label in enumerate(train_labels):\n",
    "        label_positions[label] = i\n",
    "    \n",
    "    encoded = np.zeros((len(labels), len(train_labels)))\n",
    "    for i, label in enumerate(labels):\n",
    "        j = label_positions[label]\n",
    "        encoded[i, j] = 1\n",
    "    return encoded\n",
    "\n",
    "def frenquency_encoder(train_labels, labels):\n",
    "    label_frequencies = {}\n",
    "    for label in train_labels:\n",
    "        label_frequencies[label] = label_frequencies.get(label, 0) + 1\n",
    "    \n",
    "    return [label_frequencies.get(label, 0) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array([frenquency_encoder(document.tokens, vocabulary) for document in corpus])\n",
    "train_Y = one_hot_encoder(classes, np.array([document.class_ for document in corpus]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = npdl.Model()\n",
    "\n",
    "shape = (train_X.shape[1], 64, train_Y.shape[1])\n",
    "\n",
    "model.add(layers.Dense(n_in=shape[0], n_out=500, activation=activations.ReLU()))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(shape[1], activation=activations.ReLU()))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(shape[2], activation=activations.Softmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=objectives.SCCE(), optimizer=optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1, train-[loss 1.3845, acc 0.3000]; \n",
      "iter 2, train-[loss 1.3867, acc 0.1000]; \n",
      "iter 3, train-[loss 1.3797, acc 0.3000]; \n",
      "iter 4, train-[loss 1.3506, acc 0.4000]; \n",
      "iter 5, train-[loss 1.3101, acc 0.7000]; \n",
      "iter 6, train-[loss 1.2676, acc 0.5000]; \n",
      "iter 7, train-[loss 1.3276, acc 0.4000]; \n",
      "iter 8, train-[loss 1.2525, acc 0.6000]; \n",
      "iter 9, train-[loss 1.2294, acc 0.7000]; \n",
      "iter 10, train-[loss 1.2140, acc 0.6000]; \n",
      "iter 11, train-[loss 1.1810, acc 0.9000]; \n",
      "iter 12, train-[loss 1.1208, acc 0.8000]; \n",
      "iter 13, train-[loss 1.1151, acc 0.8000]; \n",
      "iter 14, train-[loss 1.0991, acc 0.9000]; \n",
      "iter 15, train-[loss 1.1145, acc 0.8000]; \n",
      "iter 16, train-[loss 0.9561, acc 0.9000]; \n",
      "iter 17, train-[loss 1.1352, acc 0.7000]; \n",
      "iter 18, train-[loss 1.0403, acc 0.8000]; \n",
      "iter 19, train-[loss 0.9698, acc 0.8000]; \n",
      "iter 20, train-[loss 0.8863, acc 1.0000]; \n",
      "iter 21, train-[loss 0.9684, acc 0.9000]; \n",
      "iter 22, train-[loss 0.8763, acc 1.0000]; \n",
      "iter 23, train-[loss 0.8315, acc 1.0000]; \n",
      "iter 24, train-[loss 0.8112, acc 0.9000]; \n",
      "iter 25, train-[loss 0.7870, acc 1.0000]; \n",
      "iter 26, train-[loss 0.6973, acc 1.0000]; \n",
      "iter 27, train-[loss 0.8258, acc 0.9000]; \n",
      "iter 28, train-[loss 0.7894, acc 0.9000]; \n",
      "iter 29, train-[loss 0.6522, acc 0.9000]; \n",
      "iter 30, train-[loss 0.6597, acc 1.0000]; \n",
      "iter 31, train-[loss 0.5054, acc 1.0000]; \n",
      "iter 32, train-[loss 0.5934, acc 1.0000]; \n",
      "iter 33, train-[loss 0.4355, acc 1.0000]; \n",
      "iter 34, train-[loss 0.5198, acc 1.0000]; \n",
      "iter 35, train-[loss 0.5122, acc 1.0000]; \n",
      "iter 36, train-[loss 0.4542, acc 1.0000]; \n",
      "iter 37, train-[loss 0.4324, acc 1.0000]; \n",
      "iter 38, train-[loss 0.4765, acc 1.0000]; \n",
      "iter 39, train-[loss 0.4743, acc 1.0000]; \n",
      "iter 40, train-[loss 0.4423, acc 1.0000]; \n",
      "iter 41, train-[loss 0.3103, acc 1.0000]; \n",
      "iter 42, train-[loss 0.3684, acc 1.0000]; \n",
      "iter 43, train-[loss 0.3495, acc 0.9000]; \n",
      "iter 44, train-[loss 0.2323, acc 1.0000]; \n",
      "iter 45, train-[loss 0.2217, acc 1.0000]; \n",
      "iter 46, train-[loss 0.2873, acc 1.0000]; \n",
      "iter 47, train-[loss 0.2994, acc 1.0000]; \n",
      "iter 48, train-[loss 0.2457, acc 1.0000]; \n",
      "iter 49, train-[loss 0.1634, acc 1.0000]; \n",
      "iter 50, train-[loss 0.2040, acc 1.0000]; \n",
      "iter 51, train-[loss 0.2145, acc 1.0000]; \n",
      "iter 52, train-[loss 0.2614, acc 1.0000]; \n",
      "iter 53, train-[loss 0.1958, acc 1.0000]; \n",
      "iter 54, train-[loss 0.1903, acc 1.0000]; \n",
      "iter 55, train-[loss 0.1820, acc 1.0000]; \n",
      "iter 56, train-[loss 0.1452, acc 1.0000]; \n",
      "iter 57, train-[loss 0.1164, acc 1.0000]; \n",
      "iter 58, train-[loss 0.0854, acc 1.0000]; \n",
      "iter 59, train-[loss 0.0963, acc 1.0000]; \n",
      "iter 60, train-[loss 0.1483, acc 1.0000]; \n",
      "iter 61, train-[loss 0.0752, acc 1.0000]; \n",
      "iter 62, train-[loss 0.1422, acc 1.0000]; \n",
      "iter 63, train-[loss 0.0768, acc 1.0000]; \n",
      "iter 64, train-[loss 0.0996, acc 1.0000]; \n",
      "iter 65, train-[loss 0.0449, acc 1.0000]; \n",
      "iter 66, train-[loss 0.0596, acc 1.0000]; \n",
      "iter 67, train-[loss 0.0691, acc 1.0000]; \n",
      "iter 68, train-[loss 0.0983, acc 1.0000]; \n",
      "iter 69, train-[loss 0.1279, acc 1.0000]; \n",
      "iter 70, train-[loss 0.1177, acc 1.0000]; \n",
      "iter 71, train-[loss 0.0418, acc 1.0000]; \n",
      "iter 72, train-[loss 0.1024, acc 1.0000]; \n",
      "iter 73, train-[loss 0.0270, acc 1.0000]; \n",
      "iter 74, train-[loss 0.1453, acc 1.0000]; \n",
      "iter 75, train-[loss 0.0708, acc 1.0000]; \n",
      "iter 76, train-[loss 0.0314, acc 1.0000]; \n",
      "iter 77, train-[loss 0.0411, acc 1.0000]; \n",
      "iter 78, train-[loss 0.0409, acc 1.0000]; \n",
      "iter 79, train-[loss 0.0394, acc 1.0000]; \n",
      "iter 80, train-[loss 0.0470, acc 1.0000]; \n",
      "iter 81, train-[loss 0.0638, acc 1.0000]; \n",
      "iter 82, train-[loss 0.0837, acc 1.0000]; \n",
      "iter 83, train-[loss 0.0390, acc 1.0000]; \n",
      "iter 84, train-[loss 0.0711, acc 1.0000]; \n",
      "iter 85, train-[loss 0.0944, acc 1.0000]; \n",
      "iter 86, train-[loss 0.0587, acc 1.0000]; \n",
      "iter 87, train-[loss 0.0449, acc 1.0000]; \n",
      "iter 88, train-[loss 0.0430, acc 1.0000]; \n",
      "iter 89, train-[loss 0.0256, acc 1.0000]; \n",
      "iter 90, train-[loss 0.0231, acc 1.0000]; \n",
      "iter 91, train-[loss 0.0624, acc 1.0000]; \n",
      "iter 92, train-[loss 0.0183, acc 1.0000]; \n",
      "iter 93, train-[loss 0.0163, acc 1.0000]; \n",
      "iter 94, train-[loss 0.0370, acc 1.0000]; \n",
      "iter 95, train-[loss 0.0204, acc 1.0000]; \n",
      "iter 96, train-[loss 0.0193, acc 1.0000]; \n",
      "iter 97, train-[loss 0.0261, acc 1.0000]; \n",
      "iter 98, train-[loss 0.0316, acc 1.0000]; \n",
      "iter 99, train-[loss 0.0242, acc 1.0000]; \n",
      "iter 100, train-[loss 0.0184, acc 1.0000]; \n"
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query):\n",
    "    tokens = tokenize(query)\n",
    "    vector = frenquency_encoder(tokens, vocabulary)\n",
    "    prediction = model.predict([vector])\n",
    "    index = prediction.argmax()\n",
    "    return classes[index]\n",
    "\n",
    "def answer(query):\n",
    "    predicted_class = predict(query)\n",
    "    answers = next(filter(lambda intent: intent['class'] == predicted_class, intents))['responses']\n",
    "    return random.choice(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
