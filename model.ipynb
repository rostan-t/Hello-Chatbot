{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import dataclasses\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import npdl\n",
    "from npdl import activations\n",
    "from npdl import layers\n",
    "from npdl import objectives\n",
    "from npdl import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json', 'r') as file:\n",
    "    intents = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = str\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Document:\n",
    "    classs_: str\n",
    "    tokens: Iterable[Token]\n",
    "\n",
    "def tokenize(sentence: str,\n",
    "             language: str = 'english') -> Iterable[Token]:\n",
    "    stop_words = stopwords.words(language)\n",
    "    return np.array([SnowballStemmer(language).stem(word.lower()) for word in nltk.word_tokenize(sentence, language)\n",
    "                     if word not in stop_words and word.isalpha()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the **vocabulary**, the **classes** and the **corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary: List[Token] = []\n",
    "corpus: List[Document] = []\n",
    "classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in intents:\n",
    "    class_ = document['class']\n",
    "    classes.append(class_)\n",
    "    for pattern in document['patterns']:\n",
    "        tokens = tokenize(pattern)\n",
    "        vocabulary.extend(tokens)\n",
    "        corpus.append(Document(class_, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(set(vocabulary))\n",
    "classes = sorted(set(classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
